{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ce02ac-96d3-4bf9-8efe-8d10d441a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae3e01c-8e9a-4bf5-996e-51ad61025c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085d7194a199488db52e104b284bc029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = '../../meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Load our model from local\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME+'_model', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# And its associated tokenizer from local\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME+'_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bff5c7-5b09-44e6-a6aa-09b829a15a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f93805-3ae2-4f26-b519-b1fb1b9d7c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Question: What is the capital of Australia?; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Question: What is the highest peak in North Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Question: Which country is the smallest in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Question: What is the longest river in the wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geography</td>\n",
       "      <td>Question: What is the capital of Spain?; Answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Sport</td>\n",
       "      <td>Question: Who won the first ever World Cup in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Sport</td>\n",
       "      <td>Question: Who holds the record for the most go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Sport</td>\n",
       "      <td>Question: Who has won the most Balon d'Or awar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Sport</td>\n",
       "      <td>Question: Which club won the UEFA Europa Leagu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Sport</td>\n",
       "      <td>Question: Who is the NBA's all-time assists le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Type                                           Sentence\n",
       "0    Geography  Question: What is the capital of Australia?; A...\n",
       "1    Geography  Question: What is the highest peak in North Am...\n",
       "2    Geography  Question: Which country is the smallest in the...\n",
       "3    Geography  Question: What is the longest river in the wor...\n",
       "4    Geography  Question: What is the capital of Spain?; Answe...\n",
       "..         ...                                                ...\n",
       "310      Sport  Question: Who won the first ever World Cup in ...\n",
       "311      Sport  Question: Who holds the record for the most go...\n",
       "312      Sport  Question: Who has won the most Balon d'Or awar...\n",
       "313      Sport  Question: Which club won the UEFA Europa Leagu...\n",
       "314      Sport  Question: Who is the NBA's all-time assists le...\n",
       "\n",
       "[315 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "df = df.replace('\\\"', '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7145bb89-45bc-49d8-af27-89ba54976a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Geography', 'History', 'Literature', 'Music', 'Science', 'Sport'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_unique = df['Type'].unique()\n",
    "types_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7fa65b-b222-4445-9466-6dbf166f406f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "Geography     [Question: What is the capital of Australia?; ...\n",
       "History       [Question: When was the United States founded?...\n",
       "Literature    [Question: Who wrote 'To Kill a Mockingbird'?;...\n",
       "Music         [Question: Who was the 'King of Pop'?; Answer:...\n",
       "Science       [Question: What is the chemical symbol for Hyd...\n",
       "Sport         [Question: Who won the FIFA World Cup in 2018?...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('Type')['Sentence'].apply(list)\n",
    "del df\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7174b3e-ebb4-4ea8-84c5-7cca3bc17590",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_SAMPLING_OUTPUT = True\n",
    "ENABLE_NAIVE_OUTPUT = False\n",
    "ENABLE_LM_HEAD_ACTIVATION = False\n",
    "ENABLE_MODEL_LAYERS_ACTIVATION = True\n",
    "ENABLE_MODEL_LAYERS_INDEX = 31\n",
    "ENABLE_MODEL_LAYERS_NAME = 'mlp'\n",
    "ENABLE_PCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33b210c-0f60-4fb9-83a1-df0d88ab2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(layer):\n",
    "    def hook(module, input, output):\n",
    "        layer.activation = output.detach()  # we use `detach` to separate this value from the computation graph\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1631e4dd-6485-4f39-9f4b-69848d42e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_LM_HEAD_ACTIVATION:\n",
    "    hook_ref = model.lm_head.register_forward_hook(get_activation(model.lm_head))\n",
    "    input = tokenizer(\"This is a test sentence\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**input)  # Process the input\n",
    "    activation = model.lm_head.activation  # Access the activation of the lm_head layer you just hooked\n",
    "    hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf49757-d5f1-40d2-bf02-d69c9ca150c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_LM_HEAD_ACTIVATION:\n",
    "    print(activation.shape)\n",
    "    print(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd0541e-7ef9-46b1-958a-3ac025dedcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_LAYERS_ACTIVATION and ENABLE_MODEL_LAYERS_NAME == 'mlp':\n",
    "    hook_ref = model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp.register_forward_hook(get_activation(model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(grouped['Sport'][:30], return_tensors=\"pt\", padding='longest').to(device)\n",
    "    outputs = model(**inputs)\n",
    "    activation = model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp.activation\n",
    "    hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11eeab62-767e-4a73-aba7-9a0867216b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 82, 4096])\n",
      "tensor([[[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 5.3271e-01, -3.0518e-01,  3.3838e-01,  ..., -2.4817e-01,\n",
      "           3.7903e-02, -2.3633e-01],\n",
      "         [ 5.2930e-01, -3.0176e-01,  3.2300e-01,  ..., -2.2717e-01,\n",
      "           2.8107e-02, -2.5171e-01],\n",
      "         [ 5.2637e-01, -3.0127e-01,  2.9712e-01,  ..., -1.9763e-01,\n",
      "           1.5945e-02, -2.7661e-01]],\n",
      "\n",
      "        [[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 2.4658e-01, -5.8643e-01,  5.2832e-01,  ..., -4.3164e-01,\n",
      "           1.3733e-01,  1.2978e-02],\n",
      "         [ 2.4475e-01, -5.9082e-01,  5.2246e-01,  ..., -4.2725e-01,\n",
      "           1.4197e-01,  6.2513e-04],\n",
      "         [ 2.5586e-01, -6.0449e-01,  5.2197e-01,  ..., -4.3457e-01,\n",
      "           1.5027e-01, -9.7733e-03]],\n",
      "\n",
      "        [[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 4.5337e-01, -4.2090e-01,  3.2080e-01,  ..., -2.8687e-01,\n",
      "           5.0018e-02, -3.9087e-01],\n",
      "         [ 4.4897e-01, -4.1504e-01,  3.1030e-01,  ..., -2.8345e-01,\n",
      "           3.6896e-02, -3.8843e-01],\n",
      "         [ 4.4214e-01, -4.1553e-01,  3.0518e-01,  ..., -2.8125e-01,\n",
      "           2.3392e-02, -3.9282e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 4.0381e-01, -2.8735e-01,  4.1211e-01,  ..., -2.9883e-01,\n",
      "          -6.4331e-02, -2.6440e-01],\n",
      "         [ 4.1577e-01, -2.8125e-01,  4.1431e-01,  ..., -3.1323e-01,\n",
      "          -5.7587e-02, -2.6611e-01],\n",
      "         [ 4.3213e-01, -2.6929e-01,  4.1479e-01,  ..., -3.3301e-01,\n",
      "          -4.0344e-02, -2.6587e-01]],\n",
      "\n",
      "        [[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 5.6006e-01, -2.4011e-01,  2.6807e-01,  ..., -1.8091e-01,\n",
      "           1.9043e-02, -3.0688e-01],\n",
      "         [ 5.5908e-01, -2.3767e-01,  2.5073e-01,  ..., -1.7981e-01,\n",
      "           8.5831e-03, -3.1250e-01],\n",
      "         [ 5.6299e-01, -2.4146e-01,  2.4622e-01,  ..., -1.8579e-01,\n",
      "           5.6839e-03, -3.1445e-01]],\n",
      "\n",
      "        [[ 3.7842e-01,  7.3914e-02, -2.0504e-03,  ..., -2.4109e-01,\n",
      "          -2.3254e-01,  3.0566e-01],\n",
      "         [-1.5796e-01,  4.2896e-01, -1.6248e-01,  ...,  3.2983e-01,\n",
      "          -2.0740e-01, -8.5254e-01],\n",
      "         [ 5.8105e-01,  7.5781e-01,  1.5234e+00,  ...,  2.5909e-02,\n",
      "          -7.1094e-01,  1.3123e-01],\n",
      "         ...,\n",
      "         [ 5.1123e-01, -2.7783e-01,  3.9111e-01,  ..., -1.9360e-01,\n",
      "           2.7695e-02, -2.8589e-01],\n",
      "         [ 5.0977e-01, -2.7637e-01,  3.7402e-01,  ..., -1.7517e-01,\n",
      "           2.1378e-02, -2.9492e-01],\n",
      "         [ 5.1318e-01, -2.7759e-01,  3.6743e-01,  ..., -1.6895e-01,\n",
      "           2.2720e-02, -2.9297e-01]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_MODEL_LAYERS_ACTIVATION and ENABLE_MODEL_LAYERS_NAME == 'mlp':\n",
    "    print(activation.shape)\n",
    "    print(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa37bb3-58c1-4cd8-bb6a-0ff92e8b89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(activation, 'type_tensor/mlp_31_sport.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1218d4d-f2f0-486e-ba66-615b15e30d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_PCA:\n",
    "    # let's assume activation has shape [batch_size, num_tokens, num_features]\n",
    "    pca_activation = activation.cpu().numpy()\n",
    "    reshaped_activation = np.reshape(pca_activation, (pca_activation.shape[0]*pca_activation.shape[1], pca_activation.shape[2])) \n",
    "\n",
    "    dim_pca = 10\n",
    "\n",
    "    pca = PCA(n_components=dim_pca)\n",
    "    principal_components = pca.fit_transform(reshaped_activation)\n",
    "\n",
    "    # If you want to bring back the original shape for each sample and tokens\n",
    "    principal_components = principal_components.reshape(pca_activation.shape[0], pca_activation.shape[1], dim_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b5cb1aa-2c04-4304-9ff8-87393d60bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_NAIVE_OUTPUT:\n",
    "    # Assume 'output' is the final output from your model\n",
    "    output_predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted token ids\n",
    "    predicted_ids = torch.argmax(output_predictions, dim=-1)\n",
    "\n",
    "    # Convert token ids back to words\n",
    "    predicted_sentence = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "    predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afd73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=5):\n",
    "    indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d663c731-a3a6-43e3-bd74-dc15c1b17d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Who was responsible for the Great Fire of London in 1666?\n",
      "The Great Fire of London in 1666 was a devastating fire that swept through the city of London, England, killing hundreds of people and destroying thousands of buildings. The fire started on September 2, 1666, in a bakery on Pudding Lane, and quickly spread throughout the city, fueled by strong winds and closely-packed buildings.\n",
      "\n",
      "There is some debate about who was responsible for the Great Fire of London, as it is not clear what sparked the fire. However, the most commonly cited theory is that it was caused by a baker named Thomas Farriner, who left a hot oven unattended in his bakery on Pudding Lane.\n",
      "\n",
      "Farriner had been warned several times about the risk of fire in his bakery, which was located near the River Thames and surrounded by closely-packed\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_SAMPLING_OUTPUT:\n",
    "    input_sentence = \"Who was responsible for the Great Fire of London\"\n",
    "    inputs = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    generated_sentence = inputs\n",
    "    \n",
    "    MAX_LENGTH = 200\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            outputs = model(generated_sentence)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = top_k_sampling(next_token_logits)\n",
    "            generated_sentence = torch.cat((generated_sentence, next_token), dim=1)\n",
    "\n",
    "    print(tokenizer.decode(generated_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abab37-58fd-4f67-b2c4-409b3d5c4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = hooked_layer.activation.detach().cpu().numpy()\n",
    "# Save the numpy array\n",
    "np.save('activation.npy', numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005d539-fa06-45a8-801d-0782498878ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICL_env",
   "language": "python",
   "name": "icl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
