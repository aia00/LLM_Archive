{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ce02ac-96d3-4bf9-8efe-8d10d441a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d57fff-0024-4ca1-9df9-9d7f9c2f742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae3e01c-8e9a-4bf5-996e-51ad61025c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385238dd036343f9b7967e59e718c9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = '../meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Load our model from local\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME+'_model', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# And its associated tokenizer from local\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME+'_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa8a68c-43c2-463b-a47b-4af5100241cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33b210c-0f60-4fb9-83a1-df0d88ab2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(layer):\n",
    "    def hook(module, input, output):\n",
    "        layer.activation = output.detach()  # we use `detach` to separate this value from the computation graph\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1631e4dd-6485-4f39-9f4b-69848d42e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_ref = model.lm_head.register_forward_hook(get_activation(model.lm_head))\n",
    "input = tokenizer(\"This is a test sentence\", return_tensors=\"pt\").to(device)\n",
    "outputs = model(**input)  # Process the input\n",
    "activation = model.lm_head.activation  # Access the activation of the lm_head layer you just hooked\n",
    "hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf49757-d5f1-40d2-bf02-d69c9ca150c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.1039,  -0.2219,   0.3132,  ...,   1.3271,   1.8799,   0.6436],\n",
       "         [-10.1562,  -4.5742,  -4.0391,  ...,  -5.3633,  -6.7422,  -6.3516],\n",
       "         [ -5.2188,  -2.1855,   0.5386,  ...,  -1.1602,  -2.3125,  -4.0938],\n",
       "         [ -6.5625,  -3.0293,  -2.2617,  ...,  -2.8516,  -4.3047,  -3.6836],\n",
       "         [ -7.6523,  -1.6934,   1.0615,  ...,  -7.2305,  -5.2891,  -7.9023],\n",
       "         [ -2.7520,   4.0352,  11.9297,  ...,  -1.0010,  -2.3770,  -1.8301]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86ba92c0-1fee-46bc-96d7-a2628be08b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba4ff222-5ff2-448f-ad19-a852abc719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_ref = model.model.layers[0].mlp.register_forward_hook(get_activation(model.model.layers[0].mlp))\n",
    "inputs = tokenizer(\"how ar eyou\", return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)\n",
    "activation = model.model.layers[0].mlp.activation\n",
    "hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a8cb1aa-7bca-41ed-9fbb-41015a53d773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b5cb1aa-2c04-4304-9ff8-87393d60bb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unterscheidung torogyou doing'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume 'output' is the final output from your model\n",
    "output_predictions = outputs.logits\n",
    "\n",
    "# Get the predicted token ids\n",
    "predicted_ids = torch.argmax(output_predictions, dim=-1)\n",
    "\n",
    "# Convert token ids back to words\n",
    "predicted_sentence = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d663c731-a3a6-43e3-bd74-dc15c1b17d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> what color is an apple?\n",
      "\n",
      "Answer: An apple is typically a deep red color, although some varieties may be yellow, green, or a combination of colors.\n",
      "\n",
      "Here are some fun facts about apples:\n",
      "\n",
      "1. Apples are a type of fruit that belong to the rose family.\n",
      "2. There are over 7,500 varieties of apples worldwide.\n",
      "3. Apples are one of the most widely consumed fruits in the world, with over\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(logits, k=5):\n",
    "    indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "input_sentence = \"what color is an apple\"\n",
    "inputs = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "generated_sentence = inputs\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        outputs = model(generated_sentence)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = top_k_sampling(next_token_logits)\n",
    "        generated_sentence = torch.cat((generated_sentence, next_token), dim=1)\n",
    "\n",
    "print(tokenizer.decode(generated_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abab37-58fd-4f67-b2c4-409b3d5c4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = hooked_layer.activation.detach().cpu().numpy()\n",
    "# Save the numpy array\n",
    "np.save('activation.npy', numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005d539-fa06-45a8-801d-0782498878ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICL_env",
   "language": "python",
   "name": "icl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
