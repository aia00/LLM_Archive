{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36ce02ac-96d3-4bf9-8efe-8d10d441a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae3e01c-8e9a-4bf5-996e-51ad61025c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21568aa24f6454dab7bdf7d2c95b61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = '../meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Load our model from local\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME+'_model', device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# And its associated tokenizer from local\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME+'_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bff5c7-5b09-44e6-a6aa-09b829a15a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7174b3e-ebb4-4ea8-84c5-7cca3bc17590",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_SAMPLING_OUTPUT = True\n",
    "ENABLE_NAIVE_OUTPUT = False\n",
    "ENABLE_LM_HEAD_ACTIVATION = False\n",
    "ENABLE_MODEL_LAYERS_ACTIVATION = True\n",
    "ENABLE_MODEL_LAYERS_INDEX = 31\n",
    "ENABLE_MODEL_LAYERS_NAME = 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33b210c-0f60-4fb9-83a1-df0d88ab2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(layer):\n",
    "    def hook(module, input, output):\n",
    "        layer.activation = output.detach()  # we use `detach` to separate this value from the computation graph\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1631e4dd-6485-4f39-9f4b-69848d42e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_LM_HEAD_ACTIVATION:\n",
    "    hook_ref = model.lm_head.register_forward_hook(get_activation(model.lm_head))\n",
    "    input = tokenizer(\"This is a test sentence\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**input)  # Process the input\n",
    "    activation = model.lm_head.activation  # Access the activation of the lm_head layer you just hooked\n",
    "    hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecf49757-d5f1-40d2-bf02-d69c9ca150c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_LM_HEAD_ACTIVATION:\n",
    "    print(activation.shape)\n",
    "    print(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccd0541e-7ef9-46b1-958a-3ac025dedcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_MODEL_LAYERS_ACTIVATION and ENABLE_MODEL_LAYERS_NAME == 'mlp':\n",
    "    hook_ref = model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp.register_forward_hook(get_activation(model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer([\"HOW ARE YOU\", \"HOW OLD ARE YOU\"], return_tensors=\"pt\", padding='longest').to(device)\n",
    "    outputs = model(**inputs)\n",
    "    activation = model.model.layers[ENABLE_MODEL_LAYERS_INDEX].mlp.activation\n",
    "    hook_ref.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11eeab62-767e-4a73-aba7-9a0867216b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4096])\n",
      "tensor([[[ 3.7842e-01,  7.3853e-02, -2.0714e-03,  ..., -2.4109e-01,\n",
      "          -2.3291e-01,  3.0566e-01],\n",
      "         [ 1.6934e+00, -1.5552e-01, -1.2002e+00,  ...,  5.2429e-02,\n",
      "           3.4961e-01, -1.2537e-01],\n",
      "         [ 2.1543e+00, -2.4492e+00, -5.1416e-01,  ...,  4.8340e-01,\n",
      "          -5.4150e-01, -1.0791e+00],\n",
      "         ...,\n",
      "         [ 3.8809e+00, -3.6113e+00,  1.2494e-01,  ...,  5.0293e-01,\n",
      "          -4.9683e-01,  1.0303e+00],\n",
      "         [ 1.4023e+00,  4.5508e-01, -1.3562e-01,  ..., -3.4668e-01,\n",
      "          -9.8877e-01,  2.3718e-01],\n",
      "         [ 1.3252e+00,  8.3838e-01,  2.2473e-01,  ..., -2.5000e-01,\n",
      "          -1.0059e+00,  1.6479e-01]],\n",
      "\n",
      "        [[ 3.7842e-01,  7.3853e-02, -2.0714e-03,  ..., -2.4109e-01,\n",
      "          -2.3291e-01,  3.0566e-01],\n",
      "         [ 1.6934e+00, -1.5552e-01, -1.2002e+00,  ...,  5.2429e-02,\n",
      "           3.4961e-01, -1.2537e-01],\n",
      "         [ 2.1543e+00, -2.4492e+00, -5.1416e-01,  ...,  4.8340e-01,\n",
      "          -5.4150e-01, -1.0791e+00],\n",
      "         ...,\n",
      "         [ 3.6523e+00, -4.3438e+00,  1.7070e+00,  ...,  5.4688e-01,\n",
      "           1.2705e+00,  1.3076e+00],\n",
      "         [ 2.9199e+00, -2.4355e+00, -7.2449e-02,  ..., -1.9668e+00,\n",
      "          -6.1475e-01,  4.7656e-01],\n",
      "         [ 2.9277e+00, -3.2637e+00,  2.1729e-01,  ...,  4.8218e-03,\n",
      "          -7.8564e-01, -6.6797e-01]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_MODEL_LAYERS_ACTIVATION and ENABLE_MODEL_LAYERS_NAME == 'mlp':\n",
    "    print(activation.shape)\n",
    "    print(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a1f51f4-6186-4364-aee5-1413faea4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = activation.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1218d4d-f2f0-486e-ba66-615b15e30d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume activation has shape [batch_size, num_tokens, num_features]\n",
    "\n",
    "reshaped_activation = np.reshape(activation, (activation.shape[0]*activation.shape[1], activation.shape[2])) \n",
    "\n",
    "dim_pca = 10\n",
    "\n",
    "pca = PCA(n_components=dim_pca)\n",
    "principal_components = pca.fit_transform(reshaped_activation)\n",
    "\n",
    "# If you want to bring back the original shape for each sample and tokens\n",
    "principal_components = principal_components.reshape(activation.shape[0], activation.shape[1], dim_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b5cb1aa-2c04-4304-9ff8-87393d60bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_NAIVE_OUTPUT:\n",
    "    # Assume 'output' is the final output from your model\n",
    "    output_predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted token ids\n",
    "    predicted_ids = torch.argmax(output_predictions, dim=-1)\n",
    "\n",
    "    # Convert token ids back to words\n",
    "    predicted_sentence = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "    predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afd73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=5):\n",
    "    indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d663c731-a3a6-43e3-bd74-dc15c1b17d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> WHEN WAS US FOUNDED?\n",
      "When was the United States founded? The United States of America was founded on July 4, 1776, when the Continental Congress adopted the Declaration of Independence. This document declared the 13 American colonies independent from Great Britain and established the United States of America as a sovereign nation.\n",
      "\n",
      "HOW OLD IS THE USA?\n",
      "The United States of America is currently 244 years old, as of 2022. The country was founded on July 4, 1776, when the Continental Congress adopted the Declaration of Independence.\n",
      "\n",
      "HOW WAS THE USA FOUNDED?\n",
      "The United States of America was founded through a process that began in the mid-18th century. The American Revolution, which began in 1765 and lasted until 1783, was a colonial rebellion that sought to establish the United States as\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_SAMPLING_OUTPUT:\n",
    "    input_sentence = \"WHEN WAS US FOUNDED\"\n",
    "    inputs = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    generated_sentence = inputs\n",
    "    \n",
    "    MAX_LENGTH = 200\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            outputs = model(generated_sentence)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = top_k_sampling(next_token_logits)\n",
    "            generated_sentence = torch.cat((generated_sentence, next_token), dim=1)\n",
    "\n",
    "    print(tokenizer.decode(generated_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abab37-58fd-4f67-b2c4-409b3d5c4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = hooked_layer.activation.detach().cpu().numpy()\n",
    "# Save the numpy array\n",
    "np.save('activation.npy', numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005d539-fa06-45a8-801d-0782498878ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICL_env",
   "language": "python",
   "name": "icl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
