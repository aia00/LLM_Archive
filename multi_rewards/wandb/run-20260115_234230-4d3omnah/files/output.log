wandb: Detected [openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
Checkpoint tracker file does not exist: /home/ykwang/mtdata2/multi_rewards/outputs/checkpoints/static_balanced_20260115_234023/latest_checkpointed_iteration.txt
Training from scratch
test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True, 'global_steps': 0}
validation generation end
Dumped generations to /home/ykwang/mtdata2/multi_rewards/outputs/val_generations/static_balanced_20260115_234023/0.jsonl
("Initial validation metrics: {'val-core/math_multiobj/reward/mean@1': "
 "0.4775220061540604, 'val-aux/math_multiobj/score/mean@1': "
 "0.4775220000000001, 'val-aux/math_multiobj/reward_acc/mean@1': 0.0, "
 "'val-aux/math_multiobj/reward_conc/mean@1': 0.488, "
 "'val-aux/math_multiobj/reward_clar/mean@1': 0.946, 'val-aux/num_turns/min': "
 "2, 'val-aux/num_turns/max': 2, 'val-aux/num_turns/mean': 2.0, "
 "'val/reward_acc_mean': 0.0, 'val/reward_conc_mean': 0.488, "
 "'val/reward_clar_mean': 0.946}")
step:0 - val-core/math_multiobj/reward/mean@1:0.4775220061540604 - val-aux/math_multiobj/score/mean@1:0.4775220000000001 - val-aux/math_multiobj/reward_acc/mean@1:0.0 - val-aux/math_multiobj/reward_conc/mean@1:0.488 - val-aux/math_multiobj/reward_clar/mean@1:0.946 - val-aux/num_turns/min:2 - val-aux/num_turns/max:2 - val-aux/num_turns/mean:2.0 - val/reward_acc_mean:0.0 - val/reward_conc_mean:0.488 - val/reward_clar_mean:0.946
Training Progress:   0%|          | 0/31 [00:00<?, ?it/s]Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=229452, ip=172.22.3.232, actor_id=38f9682aac0d4a3bef68220901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f8034136350>)
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/single_controller/ray/base.py", line 844, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/single_controller/base/decorator.py", line 462, in inner
    return func(*args, **kwargs)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/utils/transferqueue_utils.py", line 314, in dummy_inner
    output = func(*args, **kwargs)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/utils/profiler/profile.py", line 274, in wrapper
    return func(self_instance, *args, **kwargs_inner)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/workers/fsdp_workers.py", line 926, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/workers/actor/dp_actor.py", line 740, in update_policy
    grad_norm = self._optimizer_step()
  File "/home/ykwang/projects/LLM_Archive/multi_rewards/verl/verl/workers/actor/dp_actor.py", line 415, in _optimizer_step
    self.actor_optimizer.step()
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/adam.py", line 247, in step
    adam(
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 150, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/adam.py", line 953, in adam
    func(
  File "/home/ykwang/.conda/envs/verl_vllm/lib/python3.10/site-packages/torch/optim/adam.py", line 777, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 32.25 MiB is free. Including non-PyTorch memory, this process has 47.50 GiB memory in use. Of the allocated memory 44.92 GiB is allocated by PyTorch, with 124.27 MiB allocated in private pools (e.g., CUDA Graphs), and 5.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
