{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ykwang/.conda/envs/ykwang_llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1) Extraction Code: \"Input\" section + \"Example -> Input\" section\n",
    "########################################\n",
    "def extract_input_and_example(text: str):\n",
    "    # Extract portion after \"-----Input-----\" until the next '-----' or end\n",
    "    input_pattern = r\"-----Input-----\\s*(.*?)(?=-----|$)\"\n",
    "    input_match = re.search(input_pattern, text, re.DOTALL)\n",
    "    input_section = input_match.group(1).strip() if input_match else \"\"\n",
    "\n",
    "    # Extract portion after \"-----Example-----\\nInput\\n\" until \"Output\" or next '-----' or end\n",
    "    example_pattern = r\"-----Example-----\\s*Input\\s*(.*?)(?=Output|-----|$)\"\n",
    "    example_match = re.search(example_pattern, text, re.DOTALL)\n",
    "    example_input_section = example_match.group(1).strip() if example_match else \"\"\n",
    "\n",
    "    return input_section, example_input_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2) Load the APPS dataset\n",
    "########################################\n",
    "apps_dataset = load_dataset(\"codeparrot/apps\")\n",
    "train_dataset = apps_dataset[\"train\"]\n",
    "intro_train_dataset = train_dataset.filter(lambda example: example[\"difficulty\"] == \"introductory\")\n",
    "\n",
    "# Pick a specific problem, e.g., problem index 5\n",
    "problem = intro_train_dataset[5]\n",
    "problem_desc_raw = problem[\"question\"]\n",
    "\n",
    "# We'll remove lines after \"-----Example-----\" for code generation.\n",
    "def remove_example_parts(text):\n",
    "    pattern = r\"-----Example-----.*\"\n",
    "    cleaned = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "problem_desc = remove_example_parts(problem_desc_raw)\n",
    "\n",
    "# Extract the input and example input sections\n",
    "extracted_input, extracted_example_input = extract_input_and_example(problem_desc_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]\n",
      "/home/ykwang/.conda/envs/ykwang_llama/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:823: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ykwang/.conda/envs/ykwang_llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 3) Load Models\n",
    "########################################\n",
    "\n",
    "# Model A: Code generation\n",
    "code_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name)\n",
    "code_model = AutoModelForCausalLM.from_pretrained(code_model_name, device_map=\"auto\")\n",
    "\n",
    "# Model B: Test input generation\n",
    "test_input_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Hypothetical or local path\n",
    "test_input_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    test_input_model_name,\n",
    "    use_auth_token=True\n",
    ")\n",
    "test_input_model = AutoModelForCausalLM.from_pretrained(\n",
    "    test_input_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 4) Prompt Builders\n",
    "########################################\n",
    "def build_code_prompt(question):\n",
    "    return f\"\"\"\n",
    "Please generate a complete Python script that solves the following problem.\n",
    "Enclose your final code within ```python and ``` markers.\n",
    "\n",
    "Problem Description:\n",
    "{question}\n",
    "\n",
    "Generated Code:\n",
    "\"\"\"\n",
    "\n",
    "def build_test_input_prompt(input_section, example_input_section):\n",
    "    return f\"\"\"\n",
    "Please generate another input strictly following the format described.\n",
    "\n",
    "Here is the input format:\n",
    "{input_section}\n",
    "\n",
    "Here is the example input:\n",
    "{example_input_section}\n",
    "\n",
    "Do not contain the example input and make the data seem random. Output only the generated input, line by line, with no explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 5) Generic text generation function\n",
    "########################################\n",
    "def generate_text(prompt, tokenizer, model,\n",
    "                  max_length=512, temperature=0.7, top_p=0.95, do_sample=True):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "########################################\n",
    "# 6) Extract code from LLM output\n",
    "########################################\n",
    "def extract_code(generated_text: str) -> str:\n",
    "    pattern = r\"```python\\s*(.*?)\\s*```\"\n",
    "    matches = re.findall(pattern, generated_text, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return None\n",
    "\n",
    "########################################\n",
    "# 6a) Extract input from triple backticks\n",
    "########################################\n",
    "def extract_input_from_backticks(generated_text: str) -> str:\n",
    "    pattern = r\"```(.*?)```\"\n",
    "    matches = re.findall(pattern, generated_text, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 7) Generate a single code sample (Model A)\n",
    "########################################\n",
    "def generate_code(question):\n",
    "    prompt = build_code_prompt(question)\n",
    "    raw_generation = generate_text(\n",
    "        prompt,\n",
    "        tokenizer=code_tokenizer,\n",
    "        model=code_model,\n",
    "        max_length=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    # Attempt to slice off the prompt portion\n",
    "    code_str = extract_code(raw_generation[len(prompt):])\n",
    "    return code_str\n",
    "\n",
    "########################################\n",
    "# 8) Generate a single test input (Model B)\n",
    "########################################\n",
    "def generate_single_test_input(input_section, example_input_section):\n",
    "    prompt = build_test_input_prompt(input_section, example_input_section)\n",
    "    raw_generation = generate_text(\n",
    "        prompt,\n",
    "        tokenizer=test_input_tokenizer,\n",
    "        model=test_input_model,\n",
    "        max_length=512,\n",
    "        temperature=0.4,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    # Now parse the generation for ``` blocks\n",
    "    relevant_part = raw_generation[len(prompt):]\n",
    "    extracted = extract_input_from_backticks(relevant_part)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 9) Run code with a single input\n",
    "########################################\n",
    "def run_code_with_input(code_str, single_input):\n",
    "    \"\"\"Run code_str (Python) with single_input as stdin, return (stdout, error_msg).\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tmp:\n",
    "        tmp.write(code_str)\n",
    "        tmp_filename = tmp.name\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", tmp_filename],\n",
    "            input=single_input,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        print(f\"code_str:\\n{code_str}\")\n",
    "        print(f\"result:\\n{result}\")\n",
    "        # sys.exit()\n",
    "        return (result.stdout.strip(), result.stderr)\n",
    "    except Exception as e:\n",
    "        return (None, str(e))\n",
    "    finally:\n",
    "        os.remove(tmp_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final_input looks like: 3\n",
      "13\n",
      "3 9 8 7 5 2 1 9 8 6 5 4 3\n",
      "12\n",
      "9 8 7 6 5 4 3 2 1 8 9 10\n",
      "15\n",
      "2 5 9 1 7 3 8 6 4 2 1 9 5 8 7\n",
      "code_str:\n",
      "for _ in range(int(input())):\n",
      "    n = int(input())\n",
      "    A = list(map(int, input().split()))\n",
      "    m = 10 ** 9\n",
      "    c = 0\n",
      "    for i in range(n - 1, -1, -1):\n",
      "        if A[i] <= m:\n",
      "            m = A[i]\n",
      "        else:\n",
      "            c += 1\n",
      "    print(c)\n",
      "result:\n",
      "CompletedProcess(args=['python', '/tmp/tmpp4lh8y83.py'], returncode=0, stdout='11\\n8\\n11\\n', stderr='')\n",
      "[Accepted Input #\\1]\n",
      "\\3\n",
      "13\n",
      "3 9 8 7 5 2 1 9 8 6 5 4 3\n",
      "12\n",
      "9 8 7 6 5 4 3 2 1 8 9 10\n",
      "15\n",
      "2 5 9 1 7 3 8 6 4 2 1 9 5 8 7\n",
      "\n",
      "The standard output is: \\11\n",
      "8\n",
      "11\n",
      "code_str:\n",
      "import sys\n",
      "\n",
      "t = int(sys.stdin.readline())\n",
      "for _ in range(t):\n",
      "    n = int(sys林)\n",
      "    a = list(map(int, sys.stdin.readline().split()))\n",
      "    # Compute the number of bad days\n",
      "    # To determine if a day i is bad, check if there exists a j > i such that a[j] < a[i]\n",
      "    # This can be done by keeping track of the minimum value seen so far from the end\n",
      "    min_so_far = a[-1]\n",
      "    count = 0\n",
      "    for i in range(n-2, -1, -1):\n",
      "        if a[i] > min_so_far:\n",
      "            count +=1\n",
      "        else:\n",
      "            min_so_far = a[i]\n",
      "    print(count)\n",
      "result:\n",
      "CompletedProcess(args=['python', '/tmp/tmp2ge66j6p.py'], returncode=1, stdout='', stderr='Traceback (most recent call last):\\n  File \"/tmp/tmp2ge66j6p.py\", line 5, in <module>\\n    n = int(sys林)\\nNameError: name \\'sys林\\' is not defined\\n')\n",
      "\n",
      "[Done] Saved all code + inputs + outputs to final_results.json\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 10) Main procedure\n",
    "########################################\n",
    "\n",
    "NUM_CODE_SAMPLES = 1  # We'll generate 10 code solutions\n",
    "NUM_TEST_INPUTS = 1   # We'll generate 10 test inputs (that pass standard solution)\n",
    "\n",
    "# 1) Retrieve standard solution\n",
    "# print(problem.keys())\n",
    "solutions = eval(problem.get(\"solutions\", []))\n",
    "\n",
    "if not solutions:\n",
    "    print(\"No standard solution found.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "standard_solution = solutions[0]\n",
    "\n",
    "# 2) Generate code solutions from Model A\n",
    "code_variants = []\n",
    "for i in range(NUM_CODE_SAMPLES):\n",
    "    candidate = None\n",
    "    while not candidate:\n",
    "        candidate = generate_code(problem_desc)\n",
    "    code_variants.append(candidate)\n",
    "\n",
    "# 3) Generate test inputs that pass the standard solution\n",
    "accepted_inputs = []\n",
    "standard_outputs = []  # We'll also store the standard solution's output\n",
    "attempts = 0\n",
    "\n",
    "while len(accepted_inputs) < NUM_TEST_INPUTS:\n",
    "    attempts += 1\n",
    "    raw_input = generate_single_test_input(extracted_input, extracted_example_input)\n",
    "    if raw_input == None:\n",
    "        continue\n",
    "\n",
    "    # parse first line as t, limit lines\n",
    "    lines = raw_input.split(\"\\n\")\n",
    "    if lines:\n",
    "        try:\n",
    "            t = int(lines[0].strip())\n",
    "            needed_count = 1 + 2*t\n",
    "            lines = lines[:needed_count]\n",
    "        except ValueError:\n",
    "            pass\n",
    "    final_input = \"\\n\".join(lines)\n",
    "    print(f\"Final_input looks like: {final_input}\")\n",
    "\n",
    "    # test with standard solution\n",
    "    out, err = run_code_with_input(standard_solution, final_input)\n",
    "    if err or out==\"\":\n",
    "        print(f\"Standard solution:\\n{standard_solution}\")\n",
    "        print(f\"output is '{out}'\")\n",
    "        print(f\"Attempt {attempts}: error => {err} => discarding.\")\n",
    "        continue\n",
    "\n",
    "    # If no error, record the standard solution output\n",
    "    standard_outputs.append(out)\n",
    "    accepted_inputs.append(final_input)\n",
    "    print(f\"[Accepted Input #\\{len(accepted_inputs)}]\\n\\{final_input}\\n\")\n",
    "    print(f\"The standard output is: \\{out}\")\n",
    "\n",
    "# 4) For each code variant, run on each accepted input.\n",
    "all_results = []\n",
    "\n",
    "# We'll bundle the standard outputs with each input.\n",
    "input_records = []\n",
    "for i, inp in enumerate(accepted_inputs):\n",
    "    input_records.append({\n",
    "        \"input\": inp,\n",
    "        \"standard_output\": standard_outputs[i]\n",
    "    })\n",
    "\n",
    "for idx, code_text in enumerate(code_variants, start=1):\n",
    "    code_run_results = []\n",
    "    for rec in input_records:\n",
    "        inp = rec[\"input\"]\n",
    "        std_out = rec[\"standard_output\"]\n",
    "        out, err = run_code_with_input(code_text, inp)\n",
    "        code_run_results.append({\n",
    "            \"input\": inp,\n",
    "            \"standard_solution_output\": std_out,\n",
    "            \"model_output\": out if out else \"\",\n",
    "            \"error\": err if err else None\n",
    "        })\n",
    "    all_results.append({\n",
    "        \"code_index\": idx,\n",
    "        \"code\": code_text,\n",
    "        \"test_results\": code_run_results\n",
    "    })\n",
    "\n",
    "# 5) Save to final_results.json\n",
    "with open(\"final_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=4)\n",
    "\n",
    "print(\"\\n[Done] Saved all code + inputs + outputs to final_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
