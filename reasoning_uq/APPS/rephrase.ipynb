{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ykwang/.conda/envs/ykwang_llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load the APPS dataset and filter for introductory problems.\n",
    "apps_dataset = load_dataset(\"codeparrot/apps\")\n",
    "train_dataset = apps_dataset[\"train\"]\n",
    "intro_train_dataset = train_dataset.filter(lambda example: example[\"difficulty\"] == \"introductory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load the code generation model and tokenizer.\n",
    "# code_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "code_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name)\n",
    "code_model = AutoModelForCausalLM.from_pretrained(code_model_name, device_map=\"auto\")\n",
    "\n",
    "# Load a separate rephrase model and tokenizer (using a hypothetical model name).\n",
    "rephrase_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "rephrase_tokenizer = AutoTokenizer.from_pretrained(rephrase_model_name)\n",
    "rephrase_model = AutoModelForCausalLM.from_pretrained(rephrase_model_name, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(question):\n",
    "    prompt = f\"\"\"\n",
    "Please rephrase the following problem description using different wording while preserving its original meaning. Do not change or interpret the input/output examples. Don't try to solve the problem, output only the rephrased problem description, input description, output description, along with the example input and output, formatted as follows:\n",
    "\n",
    "Problem: <rephrased problem description>\n",
    "Input description: <description of the input format>\n",
    "Output description: <description of the output format>\n",
    "Example Input: <input example>\n",
    "Example Output: <output example>\n",
    "STOP\n",
    "\n",
    "Problem Description:\n",
    "{question}\n",
    "\n",
    "Now output the rephreased one:\n",
    "\"\"\"\n",
    "    inputs = rephrase_tokenizer(prompt, return_tensors=\"pt\").to(rephrase_model.device)\n",
    "    outputs = rephrase_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=1024,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    # Decode and remove the prompt portion.\n",
    "    full_text = rephrase_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    rephrased_text = full_text[len(prompt):].strip()\n",
    "\n",
    "    # Ensure the result starts with \"Problem:\" if present.\n",
    "    start_idx = rephrased_text.find(\"Problem:\")\n",
    "    if start_idx == -1:\n",
    "        start_idx = 0\n",
    "\n",
    "    # Find \"Example Output:\" in the text.\n",
    "    idx_example_output = rephrased_text.find(\"Example Output:\")\n",
    "    if idx_example_output != -1:\n",
    "        # Find the first occurrence of \"\\n\\n\" after \"Example Output:\".\n",
    "        end_idx = rephrased_text.find(\"\\n\\n\", idx_example_output)\n",
    "        if end_idx != -1:\n",
    "            extracted = rephrased_text[start_idx:end_idx].strip()\n",
    "        else:\n",
    "            extracted = rephrased_text[start_idx:].strip()\n",
    "    else:\n",
    "        extracted = rephrased_text[start_idx:].strip()\n",
    "    \n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Build a prompt for code generation using a given question.\n",
    "def build_code_prompt(question):\n",
    "    return f\"\"\"\n",
    "Please generate a complete Python script that solves the following problem. Enclose your final code within ```python and ``` markers. Please ensure that your final code includes a main block to read input and print the result. Also, ensure that your entire output does not exceed 2048 tokens.\n",
    "\n",
    "Problem Description:\n",
    "{question}\n",
    "\n",
    "Generated Code:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Extract code from generated text (expects code to be between ```python and ```).\n",
    "def extract_code(generated_text: str) -> str:\n",
    "    pattern = r\"```python\\s*(.*?)\\s*```\"\n",
    "    matches = re.findall(pattern, generated_text, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Generate code using the code generation model.\n",
    "def generate_code(prompt):\n",
    "    inputs = code_tokenizer(prompt, return_tensors=\"pt\").to(code_model.device)\n",
    "    outputs = code_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=1024*1.5,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_text = code_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return extract_code(generated_text[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Test the generated code using the provided input/output test cases.\n",
    "# This function writes the code to a temporary file and runs it as a subprocess.\n",
    "def test_generated_code(code: str, io_pair: dict) -> float:\n",
    "    # Expect io_pair to be a dict with keys \"inputs\" and \"outputs\"\n",
    "    inputs_list = io_pair.get(\"inputs\", [])\n",
    "    outputs_list = io_pair.get(\"outputs\", [])\n",
    "    \n",
    "    if not inputs_list or not outputs_list:\n",
    "        print(\"No input/output test cases provided.\")\n",
    "        return 0.0\n",
    "\n",
    "    passed = 0\n",
    "    total = len(inputs_list)\n",
    "    \n",
    "    for inp, expected in zip(inputs_list, outputs_list):\n",
    "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tmp:\n",
    "            tmp.write(code)\n",
    "            tmp_filename = tmp.name\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"python\", tmp_filename],\n",
    "                input=inp,\n",
    "                text=True,\n",
    "                capture_output=True,\n",
    "                timeout=10\n",
    "            )\n",
    "            output = result.stdout.strip()\n",
    "            if output == expected.strip():\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"Failed:\\nInput:\\n{inp}\\nExpected:\\n{expected.strip()}\\nGot:\\n{output}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Runtime Error for input:\\n{inp}\\nError: {e}\")\n",
    "        finally:\n",
    "            os.remove(tmp_filename)\n",
    "    \n",
    "    return passed / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main procedure:\n",
    "# For a given problem, first test on the original question, then perform multiple rephrases.\n",
    "NUM_SAMPLES_ORIGINAL = 10\n",
    "NUM_REPHRASES = 3\n",
    "NUM_SAMPLES_PER_REPHRASE = 10\n",
    "\n",
    "# Pick one problem (e.g., problem index 5) from the filtered introductory dataset.\n",
    "problem = intro_train_dataset[5]\n",
    "# Convert the test case string into a dictionary (adjust if needed)\n",
    "io_pair = eval(problem[\"input_output\"])\n",
    "original_question = problem['question']\n",
    "\n",
    "# This will hold results as a list of lists:\n",
    "# The first element corresponds to original question samples,\n",
    "# and each subsequent element corresponds to one rephrase iteration.\n",
    "final_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing on Original Question ===\n",
      "\n",
      "Please generate a complete Python script that solves the following problem. Enclose your final code within ```python and ``` markers. Please ensure that your final code includes a main block to read input and print the result. Also, ensure that your entire output does not exceed 2048 tokens.\n",
      "\n",
      "Problem Description:\n",
      "Polycarp analyzes the prices of the new berPhone. At his disposal are the prices for $n$ last days: $a_1, a_2, \\dots, a_n$, where $a_i$ is the price of berPhone on the day $i$.\n",
      "\n",
      "Polycarp considers the price on the day $i$ to be bad if later (that is, a day with a greater number) berPhone was sold at a lower price. For example, if $n=6$ and $a=[3, 9, 4, 6, 7, 5]$, then the number of days with a bad price is $3$ — these are days $2$ ($a_2=9$), $4$ ($a_4=6$) and $5$ ($a_5=7$).\n",
      "\n",
      "Print the number of days with a bad price.\n",
      "\n",
      "You have to answer $t$ independent data sets.\n",
      "\n",
      "\n",
      "-----Input-----\n",
      "\n",
      "The first line contains an integer $t$ ($1 \\le t \\le 10000$) — the number of sets of input data in the test. Input data sets must be processed independently, one after another.\n",
      "\n",
      "Each input data set consists of two lines. The first line contains an integer $n$ ($1 \\le n \\le 150000$) — the number of days. The second line contains $n$ integers $a_1, a_2, \\dots, a_n$ ($1 \\le a_i \\le 10^6$), where $a_i$ is the price on the $i$-th day.\n",
      "\n",
      "It is guaranteed that the sum of $n$ over all data sets in the test does not exceed $150000$.\n",
      "\n",
      "\n",
      "-----Output-----\n",
      "\n",
      "Print $t$ integers, the $j$-th of which should be equal to the number of days with a bad price in the $j$-th input data set.\n",
      "\n",
      "\n",
      "-----Example-----\n",
      "Input\n",
      "5\n",
      "6\n",
      "3 9 4 6 7 5\n",
      "1\n",
      "1000000\n",
      "2\n",
      "2 1\n",
      "10\n",
      "31 41 59 26 53 58 97 93 23 84\n",
      "7\n",
      "3 2 1 2 3 4 5\n",
      "\n",
      "Output\n",
      "3\n",
      "0\n",
      "1\n",
      "8\n",
      "2\n",
      "\n",
      "Generated Code:\n",
      "\n",
      "\n",
      "--- Original Sample 1 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_SAMPLES_ORIGINAL):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Original Sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Code:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, code, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code:\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mgenerate_code\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code\u001b[39m(prompt):\n\u001b[1;32m      4\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m code_tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(code_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcode_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m code_tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_code(generated_text[\u001b[38;5;28mlen\u001b[39m(prompt):])\n",
      "File \u001b[0;32m~/.conda/envs/ykwang_llama/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ykwang_llama/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ykwang_llama/lib/python3.10/site-packages/transformers/generation/utils.py:3243\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3242\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3249\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ykwang_llama/lib/python3.10/site-packages/transformers/generation/utils.py:2453\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2452\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---- Test on the original question ----\n",
    "print(\"\\n=== Testing on Original Question ===\")\n",
    "original_prompt = build_code_prompt(original_question)\n",
    "print(original_prompt)\n",
    "\n",
    "original_results = []\n",
    "for s in range(NUM_SAMPLES_ORIGINAL):\n",
    "    print(f\"\\n--- Original Sample {s+1} ---\")\n",
    "    code = generate_code(original_prompt)\n",
    "    print(\"Generated Code:\\n\", code, \"\\n\")\n",
    "    if code:\n",
    "        score = test_generated_code(code, io_pair)\n",
    "        original_results.append({\"code\": code, \"success_ratio\": score})\n",
    "        print(f\"Pass Rate: {score:.2f}\")\n",
    "    else:\n",
    "        print(\"Failed to extract valid code.\")\n",
    "        original_results.append({\"code\": None, \"success_ratio\": -1})\n",
    "final_results.append(original_results)\n",
    "\n",
    "# ---- Test on rephrased questions ----\n",
    "for r in range(NUM_REPHRASES):\n",
    "    print(f\"\\n=== Rephrase {r+1} ===\")\n",
    "    rephrased_question = rephrase_question(original_question)\n",
    "    print(\"Rephrased Question:\", rephrased_question)\n",
    "    # os._exit()\n",
    "    \n",
    "    code_prompt = build_code_prompt(rephrased_question)\n",
    "    \n",
    "    rephrase_results = []  # List to store samples for this rephrase iteration.\n",
    "    for s in range(NUM_SAMPLES_PER_REPHRASE):\n",
    "        print(f\"\\n--- Sample {s+1} for Rephrase {r+1} ---\")\n",
    "        code = generate_code(code_prompt)\n",
    "        print(\"Generated Code:\\n\", code, \"\\n\")\n",
    "        if code:\n",
    "            score = test_generated_code(code, io_pair)\n",
    "            rephrase_results.append({\"code\": code, \"success_ratio\": score})\n",
    "            print(f\"Pass Rate: {score:.2f}\")\n",
    "        else:\n",
    "            print(\"Failed to extract valid code.\")\n",
    "            rephrase_results.append({\"code\": None, \"success_ratio\": -1})\n",
    "    final_results.append(rephrase_results)\n",
    "\n",
    "# Save the nested results to a JSON file.\n",
    "with open(\"final_results.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "# Optionally, print out the final nested results.\n",
    "print(\"\\nFinal Results:\")\n",
    "for i, group in enumerate(final_results):\n",
    "    label = \"Original\" if i == 0 else f\"Rephrase {i}\"\n",
    "    print(f\"\\n{label}:\")\n",
    "    for sample in group:\n",
    "        print(sample)\n",
    "\n",
    "print(\"\\nResults saved to final_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ykwang_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
